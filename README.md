# LLBChat with Ollama and Angular

Uma aplicaÃ§Ã£o Angular standalone moderna para interagir com a API do Ollama, oferecendo uma interface completa para chat com modelos de IA.

## âœ¨ CaracterÃ­sticas

- **Interface Moderna**: Design responsivo e intuitivo
- **Chat em Tempo Real**: Streaming de respostas para uma experiÃªncia fluida
- **Gerenciamento de Modelos**: Baixe, visualize e remova modelos Ollama
- **ConfiguraÃ§Ãµes AvanÃ§adas**: Ajuste parÃ¢metros como temperature, top-p, top-k
- **Tema Escuro/Claro**: Interface adaptÃ¡vel Ã s suas preferÃªncias
- **Standalone Components**: Arquitetura Angular moderna
- **TypeScript**: Totalmente tipado para melhor desenvolvimento

## ğŸš€ InstalaÃ§Ã£o e Uso

### PrÃ©-requisitos

1. **Node.js** (v18 ou superior)
2. **Angular CLI** (v17 ou superior)
3. **Ollama** instalado e rodando (https://ollama.ai)

```bash
# Instalar Angular CLI globalmente
npm install -g @angular/cli

# Verificar se o Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags
```

### ConfiguraÃ§Ã£o do Projeto

```bash
# Criar novo projeto Angular
ng new ollama-chat --standalone --routing=false --style=css

# Navegar para o diretÃ³rio
cd ollama-chat

# Instalar dependÃªncias
npm install
```

### Estrutura de Arquivos

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â””â”€â”€ chat.component.ts
â”‚   â”‚   â”œâ”€â”€ model-manager/
â”‚   â”‚   â”‚   â””â”€â”€ model-manager.component.ts
â”‚   â”‚   â””â”€â”€ settings/
â”‚   â”‚       â””â”€â”€ settings.component.ts
â”‚   â”œâ”€â”€ interfaces/
â”‚   â”‚   â””â”€â”€ ollama.interface.ts
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ollama.service.ts
â”‚   â””â”€â”€ app.component.ts
â”œâ”€â”€ main.ts
â”œâ”€â”€ styles.css
â””â”€â”€ index.html
```

### Executar a AplicaÃ§Ã£o

```bash
# Modo desenvolvimento
ng serve

# Com proxy para evitar CORS (recomendado)
ng serve --proxy-config proxy.conf.json

# Acesse http://localhost:4200
```

## ğŸ”§ ConfiguraÃ§Ã£o do Ollama

### InstalaÃ§Ã£o do Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Baixe o instalador do site oficial
```

### Baixar Modelos

```bash
# Modelos recomendados
ollama pull llama3        # Modelo geral (4.7GB)
ollama pull mistral       # Modelo rÃ¡pido (4.1GB)
ollama pull codellama     # Para cÃ³digo (3.8GB)
ollama pull phi3          # Modelo leve (2.3GB)

# Listar modelos instalados
ollama list

# Executar Ollama (se nÃ£o estiver rodando)
ollama serve
```

### Configurar CORS (se necessÃ¡rio)

Se encontrar problemas de CORS, configure o Ollama:

```bash
# Linux/macOS
export OLLAMA_ORIGINS="*"
ollama serve

# Windows
set OLLAMA_ORIGINS=*
ollama serve
```

## ğŸ› ï¸ Funcionalidades

### 1. Chat Interface

- Streaming de respostas em tempo real
- HistÃ³rico de conversas
- Suporte a mÃºltiplas linhas
- Timestamps das mensagens
- Rolagem automÃ¡tica

### 2. Gerenciador de Modelos

- Visualizar modelos instalados
- Baixar novos modelos com barra de progresso
- Remover modelos nÃ£o utilizados
- InformaÃ§Ãµes detalhadas (tamanho, famÃ­lia, parÃ¢metros)

### 3. ConfiguraÃ§Ãµes AvanÃ§adas

- URL customizada do Ollama
- ParÃ¢metros de geraÃ§Ã£o (temperature, top-p, top-k)
- Tema claro/escuro
- Exportar/importar configuraÃ§Ãµes
- InformaÃ§Ãµes do sistema

## ğŸ“¡ API do Ollama

A aplicaÃ§Ã£o utiliza as seguintes APIs:

- `GET /api/tags` - Listar modelos
- `POST /api/chat` - Chat com streaming
- `POST /api/generate` - GeraÃ§Ã£o de texto
- `POST /api/pull` - Baixar modelo
- `DELETE /api/delete` - Remover modelo

## ğŸ¨ PersonalizaÃ§Ã£o

### Temas

Modifique as variÃ¡veis CSS em `styles.css`:

```css
:root {
  --primary-color: #007bff;
  --secondary-color: #6c757d;
  --success-color: #28a745;
  --danger-color: #dc3545;
  --warning-color: #ffc107;
  --info-color: #17a2b8;
}
```

### Componentes

Todos os componentes sÃ£o standalone e podem ser facilmente customizados ou reutilizados.

## ğŸ” SoluÃ§Ã£o de Problemas

### Ollama nÃ£o estÃ¡ rodando

```bash
# Verificar status
ps aux | grep ollama

# Iniciar manualmente
ollama serve
```

### Erro de CORS

```bash
# Configurar CORS
export OLLAMA_ORIGINS="http://localhost:4200"
ollama serve
```

### Porta ocupada

```bash
# Usar porta diferente
ng serve --port 4201
```

## ğŸ“ Build para ProduÃ§Ã£o

```bash
# Build otimizado
ng build --configuration production

# Servir arquivos estÃ¡ticos
npx http-server dist/ollama-chat
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para mais detalhes.

## ğŸ”— Links Ãšteis

- [Ollama Official](https://ollama.ai)
- [Angular Documentation](https://angular.io)
- [TypeScript Documentation](https://typescriptlang.org)

---

**Desenvolvido com â¤ï¸ usando Angular e TypeScript**
